\lab{Python}{Data Structures I}{Data Structures I}
\label{lab:Python_DataStructures}
\objective{Learn about basic data structures.}

We can store data in a computer in a variety of ways.
The most basic way we have to store data is via primitive data types.
These data types are booleans, strings, floats, and integers.
Most information that we care to store is in one of these forms.
However, these primitive types can quickly become unwieldy for storing large amounts information.
Luckily, we can create more natural ways to store data by using primitive data types, along with arrays, as our building blocks.
More complex data structures are called \emph{abstract data types}.
Python possesses a couple of the more common abstract data types, including dictionaries, sets, and lists.
All of the abstract data types slow down in performance as the size of the data structure increases.

Most abstract data types use an object called a \emph{node} to store data.
A node essentially acts as a box in which we store an arbitrary piece of data.
A good way to understand an abstract data structure would be to think of it like the postal delivery system.
We start by inserting an item into the postal system.  (Because the data stored in a node is arbitrary, our item can be anything.)
The first thing that the postal system will do with our item is put it into a box.
The postal system now only has to efficiently handle boxes;
it doesn't have to worry about working with each item that goes through system.
The postal system can attach delivery specific data to any box that would allow it to be processed more effectively, 
like the return address or a little label indicating that the contents are ``fragile''.
Thus, the postal system has abstracted itself from the items it delivers.
If we didn't use nodes or data boxes, we would have to build a new tree for each data type we wanted to store.
Thus, nodes allow us to abstract data structure from the kind of data we wish to store.
A node in a binary tree will be different from a node used in a priority queue.
Nodes allow us to generalize the functionality of a data structure.

\section*{Linked Lists}
Linked lists are one of the most common and basic data structures.
Linked lists are linked together, as the name implies, and use nodes to store data.
Nodes can be inserted or removed from either end of a linked list in a constant amount of time, independent of the size of the list.
To insert or delete nodes in the middle of the list, we must first locate the correct node.
Finding this node requires that we traverse the list, which will take longer as the list size grows.
Linked lists do not allow random access like arrays.
However, they \emph{do} always have a reference that points the head, or first node, of the list.
They may also have a tail reference that points to the end, or last node, of the list.

There are several ways we can link nodes together.  The three most common types of linked lists are singly-linked, doubly-linked, and circularly-linked.
Singly-linked nodes store only a single reference that points to the next node in the list.
Doubly-linked nodes have two references:
one that points to the previous node and one that points to the next node in the list.
This allows for a doubly-linked list to be traversed in both directions, whereas a singly-linked list can only be traversed in one direction.
A circularly linked list is a singly or doubly-linked list where the tail node points to the head node as the next node.
Circularly-linked lists are commonly used as data buffers.

\subsection*{Inserting and Removing}

To insert into the end of a linked list, we need to make the old tail of the list point to the new node, increase the size counter, and update the reference to the tail of the list (if we have either).

To remove a node, since it is in the middle of the list, we need to update the reference in the node before, the reference in the node after (if it is a doubly-linked list), and decrease the size counter (if we have one).
If we are removing the last node in the list, we also need to update the reference to the tail, and if it is the only node in the list we need to set the reference to the head as None.


\begin{problem}
Implement a singly-linked list with methods for inserting, removing, and finding nodes. Insert nodes at the end of the list, but implement a method such that you can remove a node from anywhere in the list. You should also implement a method that clears the list. Below is a template.
\begin{lstlisting}
class Node():
    def __init__(self):
        self.right=None
        self.value=None
class Linkedlist():
    def __init__(self):
        self.head=None
    def insert(self,data):
        pass
    def remove(self,data):
        pass
    def find(self,data):
        pass
    def clear(self):
        self.head=None
    def __repr__(self):
        temp=self.head
        string='['
        while temp!=None:
            string+=str(temp.val)+','
            temp=temp.right
        string+=']'
        return string
\end{lstlisting}
\end{problem}

If you are familiar with other coding languages, you might be a bit worried about the \li{clear} method.
You would think that setting the reference to the head to None would cause us to lose all access to all the nodes in the list without being able to delete them!
In other languages, this would indeed cause some serious memory leaks.
However, Python is able to keep track of which variables are being used, and if there is no access to them, it automatically deletes them.
This allows us to set the reference to the head to be None and allows Python to clean up the leftover nodes.

\section*{Hash Tables}
A hash table is a very simple data structure that trades space for speed.
Most of the operations of a hash table execute in constant time, independent of the size of the data structure.
 As such, hash tables have very fast lookup times;  they form the underlying data structure of Python's dictionaries.

The heart of a hash table is a good hash function.
A hash function maps an input to a positive integer.
This positive integer is then used as an index to access the hash table.
Since the hash function must be executed to perform any operation on the hash table, it is important that the hash function executes quickly.
An ideal hash function will map unique inputs to unique outputs that are uniformly distributed over the hash space.
However, it is very difficult to create an ideal hash function.  Most hash functions will experience \emph{hash collisions}.
This is where two unique inputs yield the same hash output.
Fortunately, there are ways to handle hash collisions.  The two methods that we will discuss are open addressing and chaining.

\subsection*{Open Addressing}
The term \emph{open addressing} indicates that the output of the hash function doesn't necessary identify the location of some data.
One popular form of open addressing is probing.
We will discuss linear probing and quadratic probing.

In the event of a hash collision, linear probing seeks to resolve the collision by looking for the next available location in the hash table.
The following is an example of a linear probing hash function where $n$ is the size of the hash table, $h(x)$ is the hash function, and $i$ is a constant of the form:
\begin{equation*}
h(x, i) = h(x) + i \pmod{n}
\end{equation*}
We can do this by sequentially visiting each location and when we find an empty one, by storing our data in that location.
However, when we want to retrieve that information we will be directed to the wrong location when we use our hash function.
We then have to begin iterating through the table to look for our data.
With linear probing, elements in the table will cluster together.
Also, if our hash table is densely populated or our hash function has lots of collisions,
 we lose the efficiency of a hash table because we resort to a linear search.

Quadratic probing seeks to mitigate the issues of linear probing by spreading out hash collisions more evenly.
A quadratic probing hash function where $c_1$ and $c_2$ are constants is of the form
\begin{equation*}
h(x, i) = h(x) + c_1i + c_2i^2 \pmod{n}
\end{equation*}

\subsection*{Chaining}
Chaining is a form of closed addressing.  The output of the hash function points to the location of the data.
With chaining, each location of the hash table references a list.
When one or more pieces of information hash to the same location, it is added to that location's list.
With a good hash function, the average size of hash location's list is relatively short,
so searching the list doesn't affect the overall performance of the hash table.

\subsection*{Hash Table Details}
As a hash table fills up, its performance degrades.
When using a hash table, a \emph{load factor} is often tracked.
This load factor reveals how full the table is by calculating the ratio of the number of empty locations to total number of locations in the hash table.
When the load factor exceeds a certain threshold, we must allocate more space for the table.
To allocate more space, we must re-hash everything in the table (since most hash functions are computed using the modulo \li{(\%)} of the size of the table).
This can sometimes be a very expensive operation.
It's important to avoid resizing the table whenever possible.

\begin{problem}
Implement a hash table that uses chaining for resolving hash collisions.
When the load factor exceeds $.75$, resize the hash table so that the load factor will be below $.33$.
\end{problem}

\section*{Trees}
Trees are a rooted group of linked nodes.  There are many types of trees, but we will only cover one of the most basic ones: the binary search tree.
The binary search tree (BST) will show us the basics elements of the tree data structure.
A binary tree is a tree that has a maximum of two children per node,
has no duplicate elements, and has an ordered structure.
Trees are commonly ordered such that the left half of the tree contains elements less than the root, and the right contain elements greater than the root.

Searching a binary search tree is very fast.
Because the process of locating a node is so quick, the processes of inserting and removing nodes are as well.
Binary search trees are often used to represent the elements of a set.

A node in a binary search tree holds a (key, value) pair and references to the left and right subtrees.
This is a recursive data structure because each of the left and right subtrees are themselves BST's.
\begin{lstlisting}
class Node(object):
    def __init__(self, data):
        self.data = data
        self.left = None
        self.right = None
\end{lstlisting}
Many of the algorithms for working with binary search trees are also recursive.
A recursive function is a function that calls itself until it reaches a base case.
In the search function below, the two base cases either establish that we have the correct
node, or that we don't have a node at all.
In both these cases, we simply return what we have.
We don't have to use recursive algorithms if we don't want to.  In fact, for large trees, a recursive algorithm will consume a lot of resources.
\begin{lstlisting}
def search(data, node):
    if node is None or node.data == data:
        return node
    elif data < node.data:
        search(data, node.left)
    else:
        search(data, node.right)
\end{lstlisting}

Inserting a node into a binary search tree is simple procedure.
We recursively insert to the right if the element is greater than the current node
or insert to the left if the element is less than the current node.
Once we reach a node without a left or right subtree we add our new node as that subtree.
We never insert a non-leaf node into a BST.

Removing nodes is only slightly more complicated.
We have three cases to consider:
\begin{enumerate}
\item No child nodes.  This is the simplest case.  We simply remove the node.
\item One child node.  In this case, we have to worry about children.
If we were to just delete the node, we would lose its entire subtree.
The solution, however, is simple enough:
we promote the child to take the place of the node we are deleting.
\item Two child nodes.  We need to be a little careful in this case.
We can't simply promote a child node.  Indeed, how would we decide which child to promote?
Fortunately, we can reduce this case down to one of the previous two cases.
Suppose we are removing node, $n$, which has two children.
We first locate either the smallest node of the right subtree or the greatest node of the left subtree.
Let's call that $c$.
We swap the data of nodes $c$ and $n$ (so node $c$ now has the same data that $n$ had and vice versa).
Due to the properties of $c$, we know that it either has one child or no children and we can now simply remove $c$ using one of the simpler cases.
\end{enumerate}

\begin{problem}
Implement a binary search tree with methods for inserting, removing, and finding nodes.
Use the \li{Node} class given in the text above for your node objects.
\end{problem}

\subsection*{Balanced Trees}
Binary search trees often perform very well in searching for data.
However, the order of insertion into a binary search tree largely determines how well the tree performs.
Binary search trees work best when elements are added in a random order.
If the elements are sorted before adding to a BST, the efficiency of a BST is completely mitigated.
The resulting degenerate tree is, essentially, a linked list.


One method for solving these problems is to keep the tree balanced.
On each insertion and removal, the tree is re-balanced to maintain optimal performance.


\section*{Graphs}
We commonly use graphs in numerical computations.
There are two different data structures that can be used to represent graphs.
Each data structure has its own advantages and disadvantages.
Which one you use depends greatly on they type of graph problem you are solving.
The two ways to represent graphs are adjacency matrices and adjacency lists, which can both
 be used to represent directed or undirected graphs.

\subsection*{Adjacency Matrices}
An Adjacency matrix is two dimensional matrix used to represent a graph.
If a graph has $n$ nodes, then representing it as an adjacency matrix requires a $n \times n$ matrix.
The $ij$th entry of the adjacency matrix represents the presence of an edge between nodes $i$ and $j$.
If the entry is 0, then no edge exists between nodes $i$ and $j$, otherwise there is is an edge between nodes $i$ and $j$.
An adjacency matrix allows us to query the existence of an edge, or its edge weight, in constant time.

\subsection*{Adjacency Lists}
We can also represent graphs as a nested list of neighbors.
The $i$th index in the list contains a list of adjacent nodes to node $i$.
If node $j$ is in this list, then there exists an edge between $i$ and $j$.
We can query the neighbors of any node in the graph in constant time.
This makes algorithms that operate locally on the graph very efficient.

\begin{problem}
Implement methods that will return a list of adjacent nodes given either an adjacency list or adjacency matrix.
\end{problem}


