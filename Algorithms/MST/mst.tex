\lab{Algorithm}{Kruskal's and Prim's Algorithm}{Kruskal's and Prim's Algorithm}
\label{Ch:Kruskal}

\objective{Find a minimum spanning tree for a connected, weighted graph using Kruskal's Algorithm and Prim's Algorithm. Apply these techniques to segment an image.}

\section*{Weighted Graphs and Spanning trees}

Remember that a graph is composed of two sets: a set of nodes and a set of edges that connect the nodes.

\begin{figure}[H]
\includegraphics[width = .4\textwidth]{graph1.pdf}
\caption{An example of an undirected graph}
\label{mst:graph1}
\end{figure}

A graph is \emph{undirected} if its edges are bi-directional. This means that the edges have no specific direction in between nodes. Figure
\ref{mst:graph1} shows an example of an undirected graph.
A \emph{weighted graph} is a graph where each edge has a value associated with it.
Usually these values represent some sort of cost or distance.
A \emph{connected graph} is a graph where there is a path, or a set of edges, that connects every two nodes together.

We can represent a connected graph by a matrix.
Each row of the matrix represents a starting point and each column represents a destination.
If an edge from node \emph{i} to node \emph{j} exists, we put the weight of the edge in the \emph{i},\emph{j} entry of the matrix.
If there is no edge, we put a 0.
For the above graph in Figure \ref{mst:graph1} we generate the following matrix:

\[
A = \begin{pmatrix}
0 & 1 & 0 & 0 & 0 & 0\\
1 & 0 & 1 & 0 & 0 & 1\\
0 & 1 & 0 & 1 & 1 & 1\\
0 & 0 & 1 & 0 & 1 & 0\\
0 & 0 & 1 & 1 & 0 & 1\\
0 & 1 & 1 & 0 & 1 & 0\\
\end{pmatrix}
\]

This matrix is called an \emph{adjacency matrix}.
Note that the graph is undirected in this example and therefore the adjacency matrix is symmetric.

Now consider the graph in Figure \ref{mst:graph4}.  This graph is similar to the graph in Figure \ref{mst:graph1}, except now weights are attached to each edge.  
\begin{figure}[H]
\includegraphics[width = .4\textwidth]{graph4.pdf}
\caption{A weighted, undirected graph}
\label{mst:graph4}
\end{figure}

The adjacency matrix for this graph is 
\[
A = \begin{pmatrix}
0 & 3 & 0 & 0 & 0 & 6\\
3 & 0 & 5 & 0 & 0 & 4\\
0 & 5 & 0 & 1 & 1 & 5\\
0 & 0 & 1 & 0 & 2 & 0\\
0 & 0 & 1 & 2 & 0 & 4\\
6 & 4 & 5 & 0 & 4 & 0\\
\end{pmatrix}
\]

Another way to store the information from a graph is to make a list of the edges with their corresponding weights.
To do this for an unweighted, undirected graph, we simply create a list of the pairs of nodes that correspond to each edge.
For a weighted, undirected graph, a third value could be added to each pair of nodes to represent the corresponding weight of the edge.
A list like this for the graph in Figure \ref{mst:graph1} would look like this:

\begin{align*}
[('A', 'B'),
 ('B', 'C'),
 ('B', 'F'),
 ('C', 'D'),\\
 ('C', 'E'),
 ('C', 'F'),
 ('D', 'E'),
 ('E', 'F')]
\end{align*}

A \emph{spanning tree} of a connected, undirected graph $G$ is an undirected graph that contains all the nodes of $G$, a subset of the edges, and no cycles.
A cycle, for undirected graphs, is a path that starts and ends on the same node without crossing any edge more than once. Figure \ref{mst:graph3} shows an example of an undirected graph with a cycle in it. The cycle includes the nodes A, B, C, and F.

\begin{figure}[H]
\includegraphics[width = .4\textwidth]{graph3.pdf}
\caption{A cycle in an undirected graph}
\label{mst:graph3}
\end{figure}

A minimum spanning tree (MST) of a weighted, undirected graph is a spanning tree where the total weight is less than or equal to the total weight of every other spanning tree.
Both Kruskal's and Prim's Algorithms are methods that find a minimum spanning tree of a weighted, undirected graph.
Figure \ref{mst:graph2} shows a spanning tree of the graph shown in Figure \ref{mst:graph1}.
Figure 5.5 shows a minimum spanning tree of the graph shown in Figure \ref{mst:graph4}.

\begin{figure}[H]
\includegraphics[width = .4\textwidth]{graph2.pdf}
\caption{A spanning tree for the graph in Figure \ref{mst:graph1}.}
\label{mst:graph2}
\end{figure}

\begin{figure}[H]
\includegraphics[width = .4\textwidth]{graph5.pdf}
\caption{A MST of the graph in Figure \ref{mst:graph4}.}
\label{mst:graph5}
\end{figure}

\section*{Kruskal's algorithm}

Given a weighted, directed graph G with $n$ nodes, Kruskal's algorithm finds a minimum spanning tree by first sorting the edges by weight from smallest to largest.
Then starting with the smallest, the algorithm adds edges to the tree as long as the addition of each new edge does not create a cycle.
When $n-1$ edges have been added, the algorithm stops.

In order to avoid creating cycles while building the spanning tree, it is necessary to keep track of which portions of the tree currently lie in connected groups.
This can be done by creating a dictionary where, when the algorithm starts, each node points to itself as the ``root" of its own tree.
As we add edges to the spanning tree we will change which nodes are the ``root" nodes. This tracks which nodes are currently connected.
Doing this allows us to add edges to the spanning tree if they connect nodes which are not already connected by the current spanning tree.

We will consider the graph in Figure \ref{mst:graph4} and apply Kruskal's algorithm.
First, we initialize our spanning tree to be an empty list. Then, to track the root nodes of each connected tree, we initialize a dictionary where each node points to itself.

\begin{lstlisting}
spanning_tree = []
nodes = {'A': 'A', 'B':'B', 'C':'C', 'D':'D', 'E':'E', 'F':'F'}
\end{lstlisting}
Since there are six nodes in the graph, we will continue until we have five edges in the spanning tree.
Next, sort the edges by weight.

\begin{lstlisting}
edges = [('C', 'D', 1), ('C', 'E', 1), ('D', 'E', 2), ('A', 'B', 3), 
	     ('B', 'F', 4), ('E', 'F', 4), ('B', 'C', 5), ('C', 'F', 5), 
	     ('A', 'F', 6)]
\end{lstlisting}

Now we begin iterating through the edges to build the spanning tree.
The first edge in the list is \li{('C', 'D',1)}.
The root for \li{'C'} is \li{'C'} and the root for \li{'D'} is \li{'D'}, so we add this edge to the tree.
The tree becomes:
\begin{lstlisting}
[('C', 'D', 1)]
\end{lstlisting}
We then change the root node of \li{'D'} to be \li{'C'}.
The dictionary of root nodes now looks like:

\begin{lstlisting}
{'A': 'A', 'B':'B', 'C':'C', 'D':'C', 'E':'E', 'F':'F'}
\end{lstlisting}

Now we process the next edge, \li{('C', 'E', 1)}.
The root node of \li{'C'} is \li{'C'}, and the root node of \li{'E'} is \li{'E'}, so adding this edge does not create a cycle.
We add this edge into the tree, so the tree is now:

\begin{lstlisting}
[('C', 'D', 1), ('C', 'E', 1)]
\end{lstlisting}
Then we change the root of \li{'E'} so that it is \li{'C'}. The dictionary is now:

\begin{lstlisting}
{'A': 'A', 'B':'B', 'C':'C', 'D':'C', 'E':'C', 'F':'F'}
\end{lstlisting}

The next edge is \li{('D', 'E', 2)}.
The root node of the tree containing \li{'D'} is \li{'C'} and the root node of the tree containing \li{'E'} is \li{'C'}, so these nodes are already connected, so we do not add this edge to the spanning tree.

The next edge is \li{('A', 'B', 3)}.
The root node for \li{'A'} is \li{'A'}, and the root node for \li{'B'} is \li{'B'}, so we add the edge to the tree and update the dictionary.
The dictionary becomes:

\begin{lstlisting}
{'A': 'A', 'B':'A', 'C':'C', 'D':'C', 'E':'C', 'F':'F'}
\end{lstlisting}
The tree becomes:
\begin{lstlisting}
[('C', 'D', 1), ('C', 'E', 1), ('A', 'B', 3)]
\end{lstlisting}

The next edge is \li{('B', 'F', 4)}.
The root node for \li{'B'} is \li{'A'} and the root node for \li{'F'} is \li{'F'}, so we add the edge to the tree and change the root node of \li{'F'} to be \li{'A'}.

The dictionary becomes:

\begin{lstlisting}
{'A': 'A', 'B':'A', 'C':'C', 'D':'C', 'E':'C', 'F':'A'}
\end{lstlisting}
The tree becomes:
\begin{lstlisting}
[('C', 'D', 1), ('C', 'E', 1), ('A', 'B', 3), ('B', 'F', 4)]
\end{lstlisting}
The next edge is \li{('E', 'F', 4)}.
The root node for \li{'E'} is \li{'C'} and the root node for \li{'F'} is \li{'A'}.
We add the edge to the tree and end the algorithm since the spanning tree now has five edges.

The final spanning tree looks like:
\begin{lstlisting}
[('C', 'D', 1), ('C', 'E', 1), ('A', 'B', 3), ('B', 'F', 4), ('E', 'F', 4)]
\end{lstlisting}
If we were to continue the algorithm, we would update the root node of \li{'A'} to be \li{'C'}. The dictionary would then look like:

\begin{lstlisting}
{'A': 'C', 'C': 'C', 'B': 'A', 'E': 'C', 'D': 'C', 'F': 'A'}
\end{lstlisting}


One of the keys to this algorithm is to make sure we are not adding cycles into the spanning tree. A way to think about how the algorithm does this is by picturing it building smaller trees that have a common root node, and then as the algorithm progresses, all of these smaller trees join to have the same root node. To find the root node of the tree, we look through our dictionary until we find a node that points to itself.
For example, to find the root node of \li{'F'} we first need to get the value for \li{'F'} from the dictionary.  This is \li{'A'}.
Then we get the value for \li{'A'} from the dictionary which is \li{'C'}.
Since the value of \li{'C'} in the dictionary is itself, then \li{'C'} is the root node of the graph containing \li{'F'}.
It is necessary to trace through the dictionary in this manner each time because we avoid iterating over all of the nodes and updating their root values everytime.

Below is the outline of the algorithm we have described above:

\begin{itemize}

% Note: The comments in the solutions match the outline.
% When making changes, please keep that in mind.

\item Initialize an empty list of edges for the minimum spanning tree.

\item Make a dictionary that points each node toward its root (not always directly to it).
Start with each node pointing to itself.

\item Initialize the number of nodes that still need to be processed to the number of nodes minus 1.

\item Define a helper function that, given a node, traces through the dictionary to find the root of its tree.
This can be done like this:

	\begin{itemize}

	\item Initialize a temporary variable to be the node for which we are finding the root.

	\item While the temporary node does not point to itself in the dictionary:

		\begin{itemize}

		\item Update the temporary node to be the node it currently points to in the dictionary.

		\end{itemize}

	\item Return the temporary node.

	\end{itemize}

\item Iterate over the edges by ascending weight.
Use a \li{for} loop for this and return the tree when it is big enough which breaks the loop for you.

	\begin{itemize}

	\item Trace through the dictionary to find the root node of each of the nodes in the edge you are processing.

	\item If the roots are not the same (i.e. if adding the edge doesn't form a cycle):

		\begin{itemize}

		\item Add the edge to the tree.

		\item Lower the number of edges remaining by one.

		\item If the number of edges remaining is 0, return the tree (which also breaks the loop).

		\item Update the root of the root of the second node in the edge to be the root of the first node in the edge.
			This lets us record that the two subtrees are now connected.

		\end{itemize}

	\end{itemize}

\end{itemize}
Note on implementation: You can iterate over a sorted copy of a list using the built in \li{sorted} function.
You can sort by the third value in each tuple using the \li{itemgetter} function that is part of the \li{operator} library included with Python.
For example:
\begin{lstlisting}
from operator import itemgetter
...
for n1, n2, weight in sorted(edges, key=itemgetter(2)):
    ...
\end{lstlisting}

\begin{problem}
Implement Kruskal's algorithm.
Use the data from MSTdata.npy to test your tree. To load the data use
\begin{lstlisting}
np.load("MSTdata.npy")
\end{lstlisting}
Use the \li{formChanger} function below to put it in the right form.
\begin{lstlisting}
def formChanger(oldData):
    newData=[]
    for i in oldData: newData.append((i[0],i[1],int(i[2])))
    return newData
\end{lstlisting}
\end{problem}
\section*{Prim's algorithm}

Prim's is a similar algorithm for finding minimum spanning trees.
While it is much slower than Kruskal's algorithm for sparse graphs, it is much faster for dense graphs because Prim's algorithm avoids sorting the edges.

Again, consider the example shown in Figure \ref{mst:graph4} .
We first initialize a dictionary with all of the nodes as keys in order to track which nodes have not been processed.
At the beginning it will look like:
\begin{lstlisting}
{'A': False, 'B': False, 'C': False, 'D': False, 'E': False, 'F': False}
\end{lstlisting}
We set all of the values to \li{'False'} since none of the nodes have been processed yet.

We then form another dictionary that maps each node to the edges that contain it. For example, the node \li{'A'} is contained in the edge \li{('A', 'B', 3)} and the edge \li{('A', 'F', 6)}, so we map \li{'A'} to these edges. Since we already know that \li{'A'} is in these edges, we only need to store the other nodes and weights, which in this case would be \li{('B', 3)} and \li{('F', 6)}.
The dictionary should end up looking like:
\begin{lstlisting}
{'A': [('B', 3), ('F', 6)],
 'B': [('A', 3), ('C', 5), ('F', 4)],
 'C': [('B', 5), ('D', 1), ('E', 1), ('F', 5)],
 'D': [('C', 1), ('E', 2)],
 'E': [('C', 1), ('D', 2), ('F', 4)],
 'F': [('A', 6), ('B', 4), ('C', 5), ('E', 4)]}
\end{lstlisting}
We also initialize an empty dictionary to track the shortest edges that run between nodes we have processed and nodes we haven't. In total, we should now have three different dictionaries.

Next we find the first edge to add to our minimum spanning tree. We can choose this to be the shortest edge, or edge with the smallest weight, from any given node. In this case, we choose \li{('D', 'C', 1)} and initialize our spanning tree as the list \li{[('D', 'C', 1)]}.
We then mark \li{'D'} and \li{'C'} as processed in the dictionary that tracks which nodes have been processed. We do this by changing their values from \li{'False'} to \li{'True'}.

Next we start to build our dictionary of nodes that are one edge away from our processed nodes. This dictionary is the one we initialized above as an empty dictionary. In this case, the nodes that are one edge away from \li{'D'} and \li{'C'} are \li{'B'}, \li{'F'}, and \li{'E'}. Notice that \li{'E'} shares an edge with both \li{'D'} and \li{'C'}. To deal with this, we choose the edge connected to \li{'E'} with the smallest weight, which would be the edge corresponding to \li{'C'}. 
Our dictionary then becomes:
\begin{lstlisting}
{'B':('C', 'B', 5), 'E':('C', 'E', 1), 'F':('C', 'F', 5)}
\end{lstlisting}
Again, notice that we did not include the key/value pair \li{'E':('D', 'E', 2)} because there is a shorter edge to \li{'E'} from \li{'C'}.

Of the edges in this dictionary that can be processed next, the shortest is \li{('C', 'E', 1)}, so we add that edge to the tree and mark \li{'E'} as processed.
With \li{'E'} being processed, we can now reach \li{'F'} at a smaller cost; four now, instead of five.
After making these changes, the dictionary of edges to process becomes:
 
\begin{lstlisting}
{'B':('C', 'B', 5), 'E':('C', 'E', 1), 'F':('E', 'F', 4)}
\end{lstlisting}

Since \li{'E'} no longer needs to be processed, we can remove it from consideration.
So this dictionary now becomes:

\begin{lstlisting}
{'B':('C', 'B', 5), 'F':('E', 'F', 4)}
\end{lstlisting}

Of the edges to be processed next, \li{('E', 'F', 4)} is the shortest, so we add it to the tree and mark \li{'F'} as processed.
After performing the appropriate modifications to the dictionary of edges to be processed next, we have:
\begin{lstlisting}
{'B':('F', 'B', 4), 'A':('F', 'A', 6)}
\end{lstlisting}

Of the edges to be processed next, \li{('F', 'B', 4)} is the shortest, so we add it to the tree and mark \li{'B'} as processed.
We now have:
\begin{lstlisting}
{'A':('B', 'A', 3)}
\end{lstlisting}

The only edge to be considered is \li{('B', 'A', 3)}, so we add it to the tree.
The tree is long enough that it spans the nodes, so the algorithm is finished.

Here's an outline for a version of Prim's algorithm.
While it is not a perfectly optimized version, it is still effective.

\begin{itemize}

% Note: The comments in the solutions match the pseudocode.
% When making changes, pleas keep that in mind.

\item Initialize a dictionary to track which nodes have been processed. This will look like the dictionary shown above with nodes for keys and values set to \li{'False'}.

\item Create a dictionary that has for its key/value pairs all of the nodes for keys and the lists of edges each node is a part of for values. To do this first:
	\begin{itemize}
	\item Initialize an empty dictionary of lists.

	\item Iterate through the edges of the graph to fill the lists in the above dictionary.
		Be sure to add each edge to the lists corresponding to both of its nodes.
	\end{itemize}
\item Get the first edge to add. The shortest edge from any given node is a good pick.
\item Mark the nodes in the first edge as processed.

\item Initialize the tree to be a list containing the first edge.

\item Initialize an empty dictionary that will be used to contain the edges that can be processed next.

\item Define a helper function to insert an edge into the dictionary (if that insertion is needed).
	This can be done as follows:

	\begin{itemize}

	\item Get the value of the node that is reached by the edge.

	\item If that node isn't in the dictionary, add it, and set its value to be the edge passed to the function.

	\item If it is in the dictionary already, set its value to be the shorter of the two edges -  either the edge being processed currently or the edge that is already in the dictionary.

	\end{itemize}

\item Use the helper function to insert the edges reached by the first two processed nodes into the dictionary of edges to be processed.

\item Until the tree contains enough edges to span all the nodes:

	\begin{itemize}

	\item Find the shortest edge in the dictionary of edges to be processed.

	\item Remove the shortest edge from the dictionary.

	\item Add it to the tree.

	\item Mark the node reached by the new edge as processed.

	\item Use the helper function to insert the edges reached by the newly processed node into the dictionary of edges to be processed.

	\end{itemize}

\item Return the completed tree.

\end{itemize}

\begin{problem}
Write a Python function that uses Prim's algorithm to find the minimum spanning tree of a graph.
Test your implementation with the same data as the previous problem.
Compare the speed of Prim's algorithm with the speed of Kruskal's algorithm.
Create a function which prints these two times.
\end{problem}

Many graph packages have functions for finding the minimum spanning tree. One of these is NetworkX. You begin as follows:
\begin{lstlisting}
import networkx as nx
G=nx.Graph()
\end{lstlisting}
You can add a node $x$ with \li{G.add_node(x)} and add a edge from $x$ to $y$ with weight n with \li{G.add_edge(x,y,weight=n)}. Then \li{nx.minimum_spanning_tree(G)} returns the minimum spanning tree using Kruskal's algorithm.

\begin{problem}
Use NetworkX to find the minimum spanning tree of the data in MSTdata.npy.
Compare the output with that of your algorithm.
Create a function which prints your comparison.
\end{problem}

%Add specifications

\section*{Image Segmentation}

%Lab \ref{MSTImgSeg}

One application of MSTs is image segmentation. Image segmentation is the process that takes an image and seperates it into similar parts.
Kruskal's algorithm is especially good at this. To use Kruskal's we first convert the image we want to segment into a graph. We consider each pixel to be a node and define weights between the nodes. You then build the minimum spanning tree. If you take away the edge with the greatest weight you will get two forests (removing an edge from any graph with no cycles will create two forests). Then you can continue doing this to each forest to split up those forests. Each forest corresponds to a segment of the image.
Let $k$ be the number of divisions that is wanted and $n$ be the number of nodes.
Kruskal's algorithm is performed until $n-(k+1)$ edges are added which is the same as taking out the $k$ edges with greatest weights.

There are many different ways to turn an image into a graph and weight the edges.
A simple, yet effective, version is to make every pixel a node and the edges are the difference in intensities in the four cardinal directions.


This means that there are less than $4n$ edges.
Other image segmentation algorithms have to use $n^2$ space.
This gives the MST algorithm a critical advantage over other image segmentation algorithms.

\begin{problem}
Write a function that takes a black and white image as input and outputs a list of the edges and a list of nodes.
Store the edges using the form \li{(node,node,weight)}.
\end{problem}

The provided Kruskal's algorithm takes as inputs the list of nodes, the list of edges and the number of divisions desired.
The number of divisions often has to be higher than the actual number that is needed because sometimes one or two pixels form a division because the difference between them and the pixels around them is so great.
You will have to adjust the number of divisions until the desired result is found.  See Figure 5.6.

\begin{problem}
Perform the image segmentation algorithm on the image, then graph the original image and the three largest divisions.
(Use the Counter class from collections to find the number of pixels in each division.)
\end{problem}

\begin{problem}
Make a division of the image a different color.
\end{problem}


\vfill
\begin{figure}[ht]
\begin{minipage}[b]{0.47\linewidth}
\centering
\includegraphics[width=\textwidth]{MSTseg1.jpg}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.47\linewidth}
\centering
\includegraphics[width=\textwidth]{MSTseg2.jpg}
\end{minipage}
\begin{minipage}[b]{0.47\linewidth}
\centering
\includegraphics[width=\textwidth]{MSTseg3.jpg}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.47\linewidth}
\centering
\includegraphics[width=\textwidth]{MSTseg4.jpg}
\end{minipage}
\caption{The original image is in the top left hand corner. The three larges segments are shown in the other corners. The original image was 498x498 and 50000 divisions were used.}
\label{mst:graph6}
\end{figure}
\vfill 
